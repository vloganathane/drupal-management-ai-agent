#!/bin/bash
"""
Ollama Setup Script for Drupal AI Agent
This script helps set up Ollama with a small, efficient language model
"""

echo "ğŸ¤– Setting up Ollama for Drupal AI Agent..."
echo

# Check if Ollama is installed
if ! command -v ollama &> /dev/null; then
    echo "âŒ Ollama is not installed."
    echo "ğŸ“¦ Please install Ollama first:"
    echo "   â€¢ Visit: https://ollama.ai/"
    echo "   â€¢ Or run: curl -fsSL https://ollama.ai/install.sh | sh"
    echo
    exit 1
fi

echo "âœ… Ollama is installed"

# Check if Ollama service is running
if ! curl -s http://localhost:11434/api/tags > /dev/null 2>&1; then
    echo "ğŸš€ Starting Ollama service..."
    ollama serve &
    sleep 5
fi

# Check if the service is now running
if curl -s http://localhost:11434/api/tags > /dev/null 2>&1; then
    echo "âœ… Ollama service is running"
else
    echo "âŒ Failed to start Ollama service"
    echo "ğŸ’¡ Try running 'ollama serve' manually"
    exit 1
fi

# Pull the recommended small model
echo "ğŸ“¥ Pulling llama3.2:1b (small, efficient model)..."
if ollama pull llama3.2:1b; then
    echo "âœ… Model llama3.2:1b downloaded successfully"
else
    echo "âš ï¸  Failed to download llama3.2:1b, trying llama3.2:3b as fallback..."
    if ollama pull llama3.2:3b; then
        echo "âœ… Model llama3.2:3b downloaded successfully"
        echo "ğŸ’¡ Update OLLAMA_MODEL=llama3.2:3b in your .env file"
    else
        echo "âŒ Failed to download models. Check your internet connection."
        exit 1
    fi
fi

echo
echo "ğŸ‰ Ollama setup complete!"
echo "ğŸ“ The Drupal AI Agent is now configured to use local AI models"
echo "ğŸ”§ You can change the model by setting OLLAMA_MODEL in your .env file"
echo
echo "Available commands:"
echo "  â€¢ Test the setup: python main_modular.py execute 'create post about local AI'"
echo "  â€¢ List models: ollama list"
echo "  â€¢ Pull other models: ollama pull <model-name>"
echo
